{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d3d7c8-5eec-4753-998e-ccb0fff0aab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agnes\\anaconda3\\lib\\site-packages\\geopandas\\_compat.py:124: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "C:\\Users\\agnes\\AppData\\Local\\Temp\\ipykernel_27204\\3171183601.py:6: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import gpxpy\n",
    "import geopandas as gpd\n",
    "from shapely.ops import nearest_points\n",
    "from shapely.geometry import LineString, Point\n",
    "from shapely.ops import nearest_points\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3e75ee9b-18bd-461c-8dad-97819408369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoDataFrame wurde als C:\\Users\\agnes\\Documents\\EAGLE\\Innovative_Sensors\\FinalMeasurements\\Aufnahme_11062024_Abend_combined.gpkg gespeichert.\n",
      "GeoDataFrame wurde als C:\\Users\\agnes\\Documents\\EAGLE\\Innovative_Sensors\\FinalMeasurements\\Aufnahme_11062024_VM_combined.gpkg gespeichert.\n",
      "GeoDataFrame wurde als C:\\Users\\agnes\\Documents\\EAGLE\\Innovative_Sensors\\FinalMeasurements\\Aufnahme_12062024_Daemmerung_combined.gpkg gespeichert.\n",
      "GeoDataFrame wurde als C:\\Users\\agnes\\Documents\\EAGLE\\Innovative_Sensors\\FinalMeasurements\\Aufnahme_12062024_Mittag_combined.gpkg gespeichert.\n",
      "GeoDataFrame wurde als C:\\Users\\agnes\\Documents\\EAGLE\\Innovative_Sensors\\FinalMeasurements\\Aufnahme_13062024_Morgen_combined.gpkg gespeichert.\n",
      "GeoDataFrame wurde als C:\\Users\\agnes\\Documents\\EAGLE\\Innovative_Sensors\\FinalMeasurements\\Aufnahme_13062024_nachmittag_combined.gpkg gespeichert.\n",
      "GeoDataFrame wurde als C:\\Users\\agnes\\Documents\\EAGLE\\Innovative_Sensors\\FinalMeasurements\\Aufnahme_17062024_Abend_combined.gpkg gespeichert.\n",
      "GeoDataFrame wurde als C:\\Users\\agnes\\Documents\\EAGLE\\Innovative_Sensors\\FinalMeasurements\\Aufnahme_18062024_Mittag_combined.gpkg gespeichert.\n",
      "GeoDataFrame wurde als C:\\Users\\agnes\\Documents\\EAGLE\\Innovative_Sensors\\FinalMeasurements\\Aufnahme_19062024_Nachmittag_combined.gpkg gespeichert.\n",
      "GeoDataFrame wurde als C:\\Users\\agnes\\Documents\\EAGLE\\Innovative_Sensors\\FinalMeasurements\\Aufnahme_20062024_Vormittag_combined.gpkg gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################################################\n",
    "\n",
    "# This script is designed to load particle and GPX data from various files, \n",
    "# combine and aggregate the data, and save the aggregated data as GeoPackage files. The main steps include:\n",
    "\n",
    "# 1. Defining functions for parsing timestamps from CSV and GPX files.\n",
    "# 2. Loading particle and GPX data from the respective directories.\n",
    "# 3. Merging particle and GPX data based on timestamps.\n",
    "# 4. Calculating the mean of the particle data for each coordinate.\n",
    "# 5. Saving the aggregated data as GeoPackage files with appropriate geometries and projections.\n",
    "\n",
    "# #############################################################################################################\n",
    "\n",
    "# Function for parsing timestampsdef date_parser_csv(x):\n",
    "    return datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def date_parser_gpx(ts):\n",
    "    ts = ts + timedelta(hours=2)\n",
    "    return ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "main_folder = \"C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\\"\n",
    "\n",
    "dfs_particle = {}\n",
    "dfs_gpx = {}\n",
    "\n",
    "# Load particle data and adjust columns\n",
    "for folder_name in os.listdir(main_folder):\n",
    "    if folder_name.startswith(\"Aufnahme_\"):\n",
    "        data_folder = os.path.join(main_folder, folder_name)\n",
    "        particle_files = ['particleFront.csv', 'particleBack.csv', 'particleBottom.csv']\n",
    "        \n",
    "        particle_csv_path = os.path.join(data_folder, \"Data_0\", \"particle.csv\")\n",
    "        if os.path.exists(particle_csv_path):\n",
    "            df_particle = pd.read_csv(particle_csv_path, sep=';', parse_dates=['zeitstempel'], date_parser=date_parser_csv)\n",
    "            df_particle = df_particle.rename(columns=lambda x: f\"{x}_particleFront\" if x != 'zeitstempel' else x)\n",
    "            \n",
    "            for col in df_particle.columns:\n",
    "                if col != 'zeitstempel':\n",
    "                    df_particle[f\"{col[:-12]}_particleBack\"] = pd.NA\n",
    "                    df_particle[f\"{col[:-12]}_particleBottom\"] = pd.NA\n",
    "            \n",
    "            dfs_particle[folder_name + '_particle'] = df_particle\n",
    "        else:\n",
    "            combined_df = pd.DataFrame()\n",
    "            for particle_file in particle_files:\n",
    "                particle_csv_path = os.path.join(data_folder, \"Data_0\", particle_file)\n",
    "                if os.path.exists(particle_csv_path):\n",
    "                    df_particle = pd.read_csv(particle_csv_path, sep=';', parse_dates=['zeitstempel'], date_parser=date_parser_csv)\n",
    "                    suffix = particle_file.replace('.csv', '')\n",
    "                    df_particle = df_particle.rename(columns=lambda x: f\"{x}_{suffix}\" if x != 'zeitstempel' else x)\n",
    "                    if combined_df.empty:\n",
    "                        combined_df = df_particle\n",
    "                    else:\n",
    "                        combined_df = pd.merge(combined_df, df_particle, on='zeitstempel', how='outer')\n",
    "            dfs_particle[folder_name + '_particle'] = combined_df\n",
    "\n",
    "# Load and process GPX data\n",
    "for folder_name in os.listdir(main_folder):\n",
    "    if folder_name.startswith(\"Aufnahme_\"):\n",
    "        data_folder = os.path.join(main_folder, folder_name)\n",
    "        \n",
    "        gpx_path = None\n",
    "        for file_name in os.listdir(data_folder):\n",
    "            if file_name.lower().endswith('.gpx'):\n",
    "                gpx_path = os.path.join(data_folder, file_name)\n",
    "                break\n",
    "        \n",
    "        if gpx_path and os.path.exists(gpx_path):\n",
    "            with open(gpx_path, 'r', encoding='utf-8') as gpx_file:\n",
    "                gpx = gpxpy.parse(gpx_file)\n",
    "            \n",
    "            track_data = []\n",
    "            for track in gpx.tracks:\n",
    "                for segment in track.segments:\n",
    "                    for point in segment.points:\n",
    "                        track_data.append([point.latitude, point.longitude, point.elevation, point.time])\n",
    "            columns = ['latitude', 'longitude', 'elevation', 'zeitstempel']\n",
    "            df_gpx = pd.DataFrame(track_data, columns=columns)\n",
    "            \n",
    "            df_gpx['zeitstempel'] = pd.to_datetime(df_gpx['zeitstempel']).apply(date_parser_gpx)\n",
    "            \n",
    "            df_gpx_name = folder_name + '_track'\n",
    "            dfs_gpx[df_gpx_name] = df_gpx\n",
    "\n",
    "            \n",
    "\n",
    "# Combine the particle data with the GPX data\n",
    "for key in dfs_particle.keys():\n",
    "    folder_name = key.split('_particle')[0]\n",
    "    gpx_key = folder_name + '_track'\n",
    "    if gpx_key in dfs_gpx:\n",
    "        gdf_particle = dfs_particle[key]\n",
    "        gdf_gpx = dfs_gpx[gpx_key]\n",
    "        \n",
    "        # Konvertiere beide 'zeitstempel'-Spalten in das gleiche Format\n",
    "        gdf_particle['zeitstempel'] = pd.to_datetime(gdf_particle['zeitstempel'])\n",
    "        gdf_gpx['zeitstempel'] = pd.to_datetime(gdf_gpx['zeitstempel'])\n",
    "        \n",
    "        combined_gdf = pd.merge_asof(\n",
    "            gdf_particle.sort_values('zeitstempel'), \n",
    "            gdf_gpx.sort_values('zeitstempel'), \n",
    "            on='zeitstempel', \n",
    "            tolerance=pd.Timedelta(seconds=20)\n",
    "        )\n",
    "        \n",
    "        # Remove lines with NaN in Longitude or Latitude\n",
    "        combined_gdf = combined_gdf.dropna(subset=['longitude', 'latitude'])\n",
    "       \n",
    "        # Convert the time column to Unix timestamp (to the second)\n",
    "        combined_gdf['zeitstempel_unix'] = combined_gdf['zeitstempel'].astype('int64') // 10**9\n",
    "        \n",
    "        # Calculate the mean value of the numerical particle columns for each coordinate and the time column\n",
    "        particle_columns = combined_gdf.select_dtypes(include='number').columns.difference(['longitude', 'latitude', 'zeitstempel_unix'])\n",
    "        aggregated_gdf = combined_gdf.groupby(['longitude', 'latitude']).agg({**{col: 'mean' for col in particle_columns}, 'zeitstempel_unix': 'mean'}).reset_index()\n",
    "        \n",
    "        # Convert the Unix timestamps back to datetime\n",
    "        aggregated_gdf['zeitstempel'] = pd.to_datetime(aggregated_gdf['zeitstempel_unix'], unit='s')\n",
    "        aggregated_gdf = aggregated_gdf.drop(columns=['zeitstempel_unix'])\n",
    "        \n",
    "        # Sort by timestamp and assign the FID\n",
    "        aggregated_gdf = aggregated_gdf.sort_values('zeitstempel')\n",
    "        aggregated_gdf['fid'] = range(1, len(aggregated_gdf) + 1)\n",
    "        \n",
    "        # Convert the aggregated DataFrame into a GeoDataFrame\n",
    "        aggregated_gdf_gdf = gpd.GeoDataFrame(aggregated_gdf, geometry=gpd.points_from_xy(aggregated_gdf.longitude, aggregated_gdf.latitude))\n",
    "        \n",
    "        # Set the CRS to EPSG:4326 (WGS 84) and transform it to EPSG:32632\n",
    "        aggregated_gdf_gdf = aggregated_gdf_gdf.set_crs('EPSG:4326').to_crs('EPSG:32632')\n",
    "        \n",
    "        save_path = os.path.join(main_folder, f\"{folder_name}_combined.gpkg\")\n",
    "        aggregated_gdf_gdf.to_file(save_path, driver=\"GPKG\")\n",
    "        \n",
    "        print(f\"GeoDataFrame wurde als {save_path} gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78940aee-5c68-4a04-94ce-34010e694d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkte wurden erfolgreich in 'C:/Users/agnes/Documents/EAGLE/Innovative_Sensors/FinalMeasurements/RouteWithDistance_new_50m.gpkg' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################################################\n",
    "\n",
    "# This script processes a MultiLineString geometry representing a route, generating points at regular intervals along the route \n",
    "# and calculating the distance from the start for each point. The steps include:\n",
    "\n",
    "# 1. Loading the MultiLineString geometry from a GeoPackage file and setting the CRS to EPSG:32632.\n",
    "# 2. Defining a function to create points along the MultiLineString at specified intervals.\n",
    "# 3. Iterating over each row in the GeoDataFrame to generate points along the MultiLineString geometries.\n",
    "# 4. Creating a new GeoDataFrame for the points and adding a column for the distance from the start.\n",
    "# 5. Saving the resulting GeoDataFrame with the points and distances as a new GeoPackage file.\n",
    "\n",
    "# #############################################################################################################\n",
    "\n",
    "\n",
    "# Load the MultiLineString geometry of the route (and set the CRS to EPSG:32632)\n",
    "gdf = gpd.read_file(\"C:/Users/agnes/Documents/EAGLE/Innovative_Sensors/FinalMeasurements/komoot_route_projected_new_2.gpkg\")\n",
    "# gdf = gdf.to_crs(epsg=32632)\n",
    "\n",
    "# Function for creating points along a line\n",
    "def create_points_along_multilinestring(mline, distance):\n",
    "    points = []\n",
    "    for line in mline.geoms:\n",
    "        if not isinstance(line, LineString):\n",
    "            continue\n",
    "        total_length = line.length\n",
    "        num_points = int(total_length / distance)\n",
    "        for i in range(num_points + 1):\n",
    "            point = line.interpolate(i * distance)\n",
    "            points.append(point)\n",
    "    return points\n",
    "\n",
    "# List for saving all points created along the route\n",
    "all_points = []\n",
    "\n",
    "# Iteration over each row in the GeoDataFrame\n",
    "for idx, row in gdf.iterrows():\n",
    "    geom = row['geometry']\n",
    "    \n",
    "    # Erzeuge Punkte entlang einer MultiLineString-Geometrie\n",
    "    if geom.geom_type == 'MultiLineString':\n",
    "        points = create_points_along_multilinestring(geom, distance=50)  # Abstand von 5 Metern zwischen den Punkten\n",
    "        all_points.extend(points)\n",
    "\n",
    "# Erzeuge ein GeoDataFrame für die Punkte\n",
    "points_gdf = gpd.GeoDataFrame(geometry=all_points, crs= gdf.crs)\n",
    "\n",
    "# Hinzufügen der Entfernung vom Startpunkt als neue Spalte\n",
    "points_gdf['distance_from_start_km'] = points_gdf.index * 50 / 1000  # fid * 5 Meter in Kilometer umrechnen\n",
    "\n",
    "# Speichern des GeoDataFrames mit den Punkten und der Entfernung\n",
    "output_gpkg = \"C:/Users/agnes/Documents/EAGLE/Innovative_Sensors/FinalMeasurements/RouteWithDistance_new_50m.gpkg\"\n",
    "points_gdf.to_file(output_gpkg, driver='GPKG')\n",
    "\n",
    "# Ausgabe zur Bestätigung\n",
    "print(f\"Punkte wurden erfolgreich in '{output_gpkg}' gespeichert.\")\n",
    "\n",
    "# # Speichern des GeoDataFrame gdf mit dem neuen CRS in einer GeoPackage-Datei\n",
    "# output_gpkg = \"C:/Users/agnes/Documents/EAGLE/Innovative_Sensors/FinalMeasurements/komoot_route_projected.gpkg\"\n",
    "# # gdf.to_file(output_gpkg, driver='GPKG')\n",
    "\n",
    "# Ausgabe zur Bestätigung\n",
    "# print(f\"GeoDataFrame wurde erfolgreich in '{output_gpkg}' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa70dc6d-bc2d-4f90-853f-7f056b2a0eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\combined\\\\Aufnahme_11062024_Abend_combined.gpkg', 'C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\combined\\\\Aufnahme_11062024_VM_combined.gpkg', 'C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\combined\\\\Aufnahme_12062024_Daemmerung_combined.gpkg', 'C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\combined\\\\Aufnahme_12062024_Mittag_combined.gpkg', 'C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\combined\\\\Aufnahme_13062024_Morgen_combined.gpkg', 'C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\combined\\\\Aufnahme_13062024_nachmittag_combined.gpkg', 'C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\combined\\\\Aufnahme_17062024_Abend_combined.gpkg', 'C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\combined\\\\Aufnahme_18062024_Mittag_combined.gpkg', 'C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\combined\\\\Aufnahme_19062024_Nachmittag_combined.gpkg', 'C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\combined\\\\Aufnahme_20062024_Vormittag_combined.gpkg']\n",
      "Standardisierung abgeschlossen und Ergebnisse gespeichert für Datei: Aufnahme_11062024_Abend_combined_MedianStandardized_withNA.gpkg\n",
      "Standardisierung abgeschlossen und Ergebnisse gespeichert für Datei: Aufnahme_11062024_VM_combined_MedianStandardized_withNA.gpkg\n",
      "Standardisierung abgeschlossen und Ergebnisse gespeichert für Datei: Aufnahme_12062024_Daemmerung_combined_MedianStandardized_withNA.gpkg\n",
      "Standardisierung abgeschlossen und Ergebnisse gespeichert für Datei: Aufnahme_12062024_Mittag_combined_MedianStandardized_withNA.gpkg\n",
      "Standardisierung abgeschlossen und Ergebnisse gespeichert für Datei: Aufnahme_13062024_Morgen_combined_MedianStandardized_withNA.gpkg\n",
      "Standardisierung abgeschlossen und Ergebnisse gespeichert für Datei: Aufnahme_13062024_nachmittag_combined_MedianStandardized_withNA.gpkg\n",
      "Standardisierung abgeschlossen und Ergebnisse gespeichert für Datei: Aufnahme_17062024_Abend_combined_MedianStandardized_withNA.gpkg\n",
      "Standardisierung abgeschlossen und Ergebnisse gespeichert für Datei: Aufnahme_18062024_Mittag_combined_MedianStandardized_withNA.gpkg\n",
      "Standardisierung abgeschlossen und Ergebnisse gespeichert für Datei: Aufnahme_19062024_Nachmittag_combined_MedianStandardized_withNA.gpkg\n",
      "Standardisierung abgeschlossen und Ergebnisse gespeichert für Datei: Aufnahme_20062024_Vormittag_combined_MedianStandardized_withNA.gpkg\n",
      "Alle Standardisierungen abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################################################\n",
    "# This script standardizes particle measurement data by:\n",
    "#\n",
    "# 1. Locating all .gpkg files ending with 'combined' in a specified directory.\n",
    "# 2. Loading route points from a specified GeoPackage file.\n",
    "# 3. Iterating over each measurement file to:\n",
    "#    - Load particle measurement points.\n",
    "#    - Identify columns starting with 'Particles'.\n",
    "#    - Initialize new columns in the result DataFrame for median particle measurements and timestamps.\n",
    "#    - Create buffer zones around each route point and find measurement points within each buffer.\n",
    "#    - Calculate and store median values of particle measurements and timestamps for each buffer zone.\n",
    "# 4. Saving the processed data with median values as new GeoPackage files.\n",
    "# 5. Printing the status of each processed file.\n",
    "# #############################################################################################################\n",
    "\n",
    "# Define the directory containing the files\n",
    "folder = 'C:\\\\Users\\\\agnes\\\\Documents\\\\EAGLE\\\\Innovative_Sensors\\\\FinalMeasurements\\\\combined\\\\'\n",
    "\n",
    "# Find all .gpkg files ending with 'combined'\n",
    "gpkg_files = glob.glob(os.path.join(folder, '**', '*combined.gpkg'), recursive=True)\n",
    "\n",
    "# List to store found file paths\n",
    "measurement_files = [file for file in gpkg_files]\n",
    "print(measurement_files)\n",
    "\n",
    "# Load route points (Komoot route)\n",
    "route_points = gpd.read_file('C:/Users/agnes/Documents/EAGLE/Innovative_Sensors/FinalMeasurements/RouteWithDistance_new_50m.gpkg')\n",
    "\n",
    "# Buffer size in meters\n",
    "buffer_size = 25\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file in measurement_files:\n",
    "    # Load particle measurement points\n",
    "    particle_gdf = gpd.read_file(file)\n",
    "    \n",
    "    # Find all columns starting with 'Particles'\n",
    "    particle_columns = [col for col in particle_gdf.columns if col.startswith('Particles')]\n",
    "    \n",
    "    # DataFrame for the results\n",
    "    result_df = route_points.copy()\n",
    "    \n",
    "    # Initialize new columns for median in result_df\n",
    "    for col in particle_columns:\n",
    "        result_df[f'median_{col}'] = np.nan\n",
    "    \n",
    "    # Initialize new column for the median timestamp\n",
    "    result_df['median_zeitstempel'] = pd.NaT\n",
    "    \n",
    "    # Convert 'zeitstempel' column to datetime\n",
    "    particle_gdf['zeitstempel'] = pd.to_datetime(particle_gdf['zeitstempel'], errors='coerce')\n",
    "    \n",
    "    # Iterate over each point in route_points\n",
    "    for idx, route_point in result_df.iterrows():\n",
    "        # Create buffer around the current route point\n",
    "        buffer = route_point.geometry.buffer(buffer_size)\n",
    "        \n",
    "        # Find measurement points within the buffer\n",
    "        points_within_buffer = particle_gdf[particle_gdf['geometry'].within(buffer)]\n",
    "        \n",
    "        # If there are measurement points within the buffer, calculate the median\n",
    "        if not points_within_buffer.empty:\n",
    "            for col in particle_columns:\n",
    "                median_value = points_within_buffer[col].median()\n",
    "                result_df.at[idx, f'median_{col}'] = median_value\n",
    "            \n",
    "            # Calculate the median timestamp\n",
    "            median_time = points_within_buffer['zeitstempel'].median()\n",
    "            result_df.at[idx, 'median_zeitstempel'] = median_time\n",
    "    \n",
    "    # # Remove rows where all median columns are None\n",
    "    # mean_columns = [f'mean_{col}' for col in particle_columns] + ['mean_zeitstempel']\n",
    "    # result_df = result_df.dropna(subset=mean_columns, how='all')\n",
    "    \n",
    "    # Convert 'median_zeitstempel' column to string before saving\n",
    "    result_df['median_zeitstempel'] = result_df['median_zeitstempel'].astype(str)\n",
    "    \n",
    "    # Convert 'median_' columns to float\n",
    "    for col in particle_columns:\n",
    "        result_df[f'median_{col}'] = result_df[f'median_{col}'].astype(float)\n",
    "    \n",
    "    # Generate the output file name based on the input file name\n",
    "    output_filename = os.path.splitext(os.path.basename(file))[0] + '_MedianStandardized_withNA.gpkg'\n",
    "    output_path = os.path.join('C:/Users/agnes/Documents/EAGLE/Innovative_Sensors/FinalMeasurements/MedianStandardized_withNA', output_filename)\n",
    "    \n",
    "    # Save the results to a new file\n",
    "    result_df.to_file(output_path, driver='GPKG')\n",
    "\n",
    "    print(f\"Standardization completed and results saved for file: {output_filename}\")\n",
    "\n",
    "print(\"All standardizations completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e757119d-549a-490e-8cd0-7b47f003c27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Aggregierte Daten wurden erfolgreich als GeoPackage unter C:/Users/agnes/Documents/EAGLE/Innovative_Sensors/FinalMeasurements/median_PM.gpkg gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################################################\n",
    "# This script calculates a median over all measurements by performing the following steps:\n",
    "#\n",
    "# 1. Locate all .gpkg files ending with '_MedianStandardized_withNA.gpkg' in a specified directory.\n",
    "# 2. Load these GeoPackage files into GeoDataFrames and combine them into a single GeoDataFrame.\n",
    "# 3. Identify columns related to particle measurements for aggregation.\n",
    "# 4. Calculate the median values for these columns grouped by 'distance_from_start_km'.\n",
    "# 5. Merge these median values with the corresponding geometry from the first GeoDataFrame.\n",
    "# 6. Sort the columns to a desired order.\n",
    "# 7. Save the aggregated GeoDataFrame, containing the median values of all measurements, as a new GeoPackage file.\n",
    "# 8. Print the status of the saved file.\n",
    "# #############################################################################################################\n",
    "\n",
    "# Directory with the standardized files\n",
    "folder = 'C:/Users/agnes/Documents/EAGLE/Innovative_Sensors/FinalMeasurements/MedianStandardized_withNA/'\n",
    "\n",
    "# Find all .gpkg files ending with '_standardized_withNA.gpkg'\n",
    "standardized_files = glob.glob(os.path.join(folder, '*_MedianStandardized_withNA.gpkg'))\n",
    "\n",
    "# List to save GeoDataFrames for each GeoPackage file\n",
    "gdfs = []\n",
    "\n",
    "# Load GeoPackage files\n",
    "for file in standardized_files:\n",
    "    gdf = gpd.read_file(file)\n",
    "    gdfs.append(gdf)\n",
    "    \n",
    "# Create total GeoDataFrame\n",
    "gdf_total = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=gdfs[0].crs)\n",
    "\n",
    "# List of columns to be aggregated\n",
    "columns_to_aggregate = [col for col in gdf_total.columns if col.startswith('median_Particles')]\n",
    "print(len(columns_to_aggregate))\n",
    "\n",
    "# Calculate Median for every point\n",
    "gdf_median = gdf_total.groupby(['distance_from_start_km'])[columns_to_aggregate].median().reset_index()\n",
    "\n",
    "# Extract the 'distance_from_start_km' and 'geometry' from the first GeoDataFrame\n",
    "geometry_df = gdfs[0][['distance_from_start_km', 'geometry']].drop_duplicates()\n",
    "\n",
    "# Merge the mean values with the 'geometry' column\n",
    "gdf_median = pd.merge(gdf_median, geometry_df, on='distance_from_start_km', how='left')\n",
    "\n",
    "# Convert the Pandas DataFrame gdf_mean into a GeoDataFrame\n",
    "gdf_median_geo = gpd.GeoDataFrame(gdf_median, geometry='geometry', crs=gdf_total.crs)\n",
    "\n",
    "# Sort the columns\n",
    "desired_columns = ['distance_from_start_km', 'geometry']\n",
    "for size in ['0.3um', '0.5um', '1.0um', '2.5um', '5.0um', '10.0um']:\n",
    "    for pos in ['Front', 'Bottom', 'Back']:\n",
    "        for col in columns_to_aggregate:\n",
    "            if f'Particles > {size} / 0.1L air_particle{pos}' in col:\n",
    "                desired_columns.append(col)\n",
    "\n",
    "gdf_median_sorted = gdf_median_geo[desired_columns]\n",
    "\n",
    "# Save output as GeoPackage\n",
    "output_gpkg = 'C:/Users/agnes/Documents/EAGLE/Innovative_Sensors/FinalMeasurements/median_PM.gpkg'\n",
    "gdf_median_sorted.to_file(output_gpkg, driver='GPKG')\n",
    "\n",
    "print(f\"Aggregated data was successfully saved as a GeoPackage under {output_gpkg}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
